# Improved Snakefile for NLRtracker pipeline
import os
from snakemake.io import glob_wildcards

configfile: "config.yaml"

# Parse annotation information
annotate = config['annotate'].split(",")
annotate_sample = annotate[0]
annotate_path = annotate[1]

# Define the final output
FINAL_OUTPUT = "done.txt"

# Function to get samples and fasta paths
def get_fa_samples(file):
    samples = []
    fa = []
    with open(file, 'r') as input:
        for line in input:
            line = line.rstrip()
            items = line.split(",")
            samples.append(items[0])
            fa.append(items[1])
    return fa, samples

# Function to map sample to fasta path
def sample_to_fa(sample, fastas, samples):
    return fastas[samples.index(sample)]

SAMPLES = annotate_sample

# Main rule to ensure correct order of execution
rule all:
    input: FINAL_OUTPUT

# Rule to run Helixer
rule run_helixer:
    input:
        fasta=f"{annotate_path}"
    output:
        helixer=config['scratch'] + f"/{annotate_sample}/helixer/{annotate_sample}_helixer.gff"
    params:
        lineage="land_plant",
        species=config['species'],
        model_path="/tsl/data/helixer/models/land_plant/land_plant_v0.3_m_0100.h5",
        subsequence_length=64152,
        additional_options=config.get('helixer_options', ''),
        mem="32G",
        queue="tsl-short"
    threads: 16
    shell:
        """
        bash scripts/helixer.sh {input.fasta} {params.lineage} {params.species} \
            {output.helixer} {params.model_path} {params.subsequence_length} {params.additional_options}
        """

# Rule to run gffread
rule run_gffread:
    input:
        fasta=f"{annotate_path}",
        helixer=rules.run_helixer.output.helixer
    output:
        gffread=config['scratch'] + f"/{annotate_sample}/gffread/{annotate_sample}_gffread.fasta"
    params:
        mem="32G",
        queue="tsl-short"
    threads: 16
    shell:
        """
        bash scripts/gffread.sh {input.fasta} {output.gffread} {input.helixer}
        """

# Rule to create new samples file
rule create_new_samples:
    input:
        gffread=rules.run_gffread.output.gffread
    output:
        new_samples=config['scratch'] + "/new_samples.csv"
    params:
        mem="32G",
        queue="tsl-short"
    threads: 16
    shell:
        """
        echo {annotate_sample},$(readlink -f {input.gffread}) > {output.new_samples}
        """

# Rule to strip asterisks
rule strip_asterisks:
    input:
        samples_file=rules.create_new_samples.output.new_samples
    output:
        no_asterisk=config['scratch'] + "/{sample}/no_asterisk.fa"
    params:
        mem="4G",
        queue="tsl-short"
    run:
        FASTAS, SAMPLES = get_fa_samples(input.samples_file)
        for sample in SAMPLES:
            fasta_path = sample_to_fa(sample, FASTAS, SAMPLES)
            shell(f"cat {{fasta_path}} | sed 's/*//g' > {{output.no_asterisk}}")

# Checkpoint for interpro
checkpoint interpro:
    input:
        config["scratch"] + "/{sample}/no_asterisk.fa"
    output:
        fa=directory(config["scratch"] + "/{sample}/ipro_fa/")
    params:
        mem="4G",
        queue="tsl-short",
        seqs_per_file=config['seqs_per_file']
    threads: 1
    shell: "bash scripts/split_fa.sh {input} {params.seqs_per_file} {output}"

# Rule to run interpro on split files and aggregate results
rule run_interpro:
    input:
        fa=config["scratch"] + "/{sample}/ipro_fa/"
    output:
        gff=config["scratch"] + "/{sample}/ipro_gff/",
        result="{sample}/results/interpro_result.gff"
    params:
        mem="32G",
        queue="tsl-short",
        tmp=config["scratch"] + "/{sample}/ipro_tmp"
    threads: 16
    run:
        ipro_files = glob_wildcards(os.path.join(input.fa, "{n}.fa")).n
        for n in ipro_files:
            shell(f"bash scripts/interpro.sh {input.fa}/{n}.fa {params.tmp} {threads} {output.gff}/{n}.gff")
        shell(f"cat {output.gff}/*.gff > {output.result}")

# Rule to run FIMO
rule run_fimo:
    input:
        samples_file=rules.create_new_samples.output.new_samples
    output:
        fimo="{sample}/results/fimo_out/fimo.gff"
    params:
        mem="32G",
        queue="tsl-short"
    threads: 16
    run:
        FASTAS, SAMPLES = get_fa_samples(input.samples_file)
        for sample in SAMPLES:
            fasta_path = sample_to_fa(sample, FASTAS, SAMPLES)
            shell(f"bash scripts/fimo.sh {sample}/results/fimo_out lib/meme.xml {{fasta_path}}")

# Rule to run HMMER
rule run_hmmer:
    input:
        samples_file=rules.create_new_samples.output.new_samples
    output:
        hmmer="{sample}/results/CJID.txt"
    params:
        mem="32G",
        queue="tsl-short"
    threads: 16
    run:
        FASTAS, SAMPLES = get_fa_samples(input.samples_file)
        for sample in SAMPLES:
            fasta_path = sample_to_fa(sample, FASTAS, SAMPLES)
            shell(f"bash scripts/hmmer.sh {{output.hmmer}} lib/abe3069_Data_S1.hmm {{fasta_path}}")

# Rule to run NLRtracker
rule run_nlrtracker:
    input:
        interpro="{sample}/results/interpro_result.gff",
        fimo="{sample}/results/fimo_out/fimo.gff",
        hmmer="{sample}/results/CJID.txt",
        samples_file=rules.create_new_samples.output.new_samples
    output:
        nlrtracker="{sample}/nlrtracker_out/done.txt"
    params:
        mem="32G",
        queue="tsl-short"
    threads: 16
    run:
        FASTAS, SAMPLES = get_fa_samples(input.samples_file)
        for sample in SAMPLES:
            fasta_path = sample_to_fa(sample, FASTAS, SAMPLES)
            shell(f"bash scripts/run_tracker.sh lib/interproscan_v5.69-101.0_entry.list {input.interpro} {input.fimo} {{fasta_path}} {sample}/nlrtracker_out {sample} p {input.hmmer} lib/iTOL_NLR_template.txt")

# Rule to finalize the pipeline
rule finalize:
    input:
        expand("{sample}/nlrtracker_out/done.txt", sample=SAMPLES)
    output:
        touch(FINAL_OUTPUT)
    params:
        mem="4G",
        queue="tsl-short"
    threads: 1
    shell:
        "cat {input} > {output}"